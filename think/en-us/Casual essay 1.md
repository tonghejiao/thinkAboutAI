The current ai is more like developing some "mapping machine".

Neural network we treat it as a black box, do not consider the internal principle, only consider the input and output, then it is a "mapping machine" as follows.

A1 -> B1
A2 -> B2
A3 -> B3
...

ChatGPT, viewed in the same way, is slightly different from a neural network, a "mapper" of outputs plus a probability distribution.

A1 -> (40% probability output) B1
A1 -> (30% probability output) B2
A2 -> (30% probability output) B3
...

This is more like the human brain's intuition about something, once the "mapper" is built, it doesn't think much about it when it is used.

Another function of the human brain is to use the "mapping machine" like a building block. This may mimic the process of reasoning in the human brain.

Another point is that the human brain does not need such a large amount of data to train it when building a "mapping machine". I think there are two reasons.

- The human brain uses old "mappers" to build new ones
- The human brain also uses reasoning when adjusting the weight of the mapper to rule out false weights

The process of humans "understanding" one thing may be the process of building multiple "mappers." When we get a "target", our brain tries to build a "mapper" and use the "mapper". Note that the use of this process will have a similar process of building blocks, and eventually find a path to reach the goal.