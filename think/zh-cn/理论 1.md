假设给这个世界的一切各自一个编号,如: 1,2,3,4...

他们之间存在一些映射关系,如:1 -> 2,3 -> 4

如果存在一台机器,输入一个编号,就能按照映射关系输出对应编号,那么他就是拥有全部智能的

关键在于怎么编码,因为我们要保证相同的对象编码也相同,不同的对象编码不同.

有些对象虽然是不同的,但是在某些前提条件下,可是认为他们是相同的.所以可以创建这种条件下专用的映射机.这可能就是举一反三能力的一种来源



自然语言就是一种编码方式，向量也是一种。

使用向量来编码有两点好处
- 多角度看待同一个对象
	- 每个维度就是一个角度，向量就像一个人的简历，虽然简历不能让我们完整了解一个人，但是可以帮我们排除错误答案，当然前提是简历上信息是真实的。我们只能这样认识世界，毕竟不是上帝。
- 相似度高的对象在多维空间上距离更近。
	- 这句话的逆命题可能不成立。多维空间上距离更近的点相似度就更高吗？

我们的大脑应该不是一个单独的大模型，而是由前额叶做中枢，协同调用各个小模型，来共同运作。不同的情况下要调用不同的小模型，这个由前额叶来决定。有些情况下需要创建模型，这也是我们人类学习的过程。这种不同情况下要做不同的事，本身就是一种映射关系，现在问题的关键就在于怎么搞出来这个模型。

人永远都在不停预测下一步要做什么，我们是一台永不停歇的预测下一步的模型。
也不一定，也许得先有目的才行。

moe的原理就是,一些问题让张三做,一些问题让李四做,谁做的最好就以谁的为准.但是在训练的过程中,路由层是怎么知道谁做的最好的?
上面说的可能不对，可能是预测下一个词不需要太多神经元参与，或者说参与者多到一定程度边际效用很小，不如各司其职。
上面说的不对,主要原因还是,少量神经元在处理某一类问题的时候.可能会超过大量神经元的模型,当然其他问题他肯定回答的不如后者,但是路由层避开他就行了.

人类是有能力检查已经输出的思路的正确性的.

我们在做证明题的时候,会使用一些方法或者公理定理,同时还有自己的直觉.他们本质上都是一些映射函数,只不过这些映射函数比较可靠.也就是说我们会维护一些比较可靠的映射函数,用于逻辑推理.但是前提是我们能准确理解这些公理定理,这就需要能准确理解自然语言的能力,这种准确理解的能力背后就是一堆映射函数支撑的.我们又是怎么创建好这些映射函数的?
一旦遇到反例,我们常常就会给自己总结的道理加上前提

人类总结的这些方法其实是人类追求严谨的结果,所以还是要实现一个目的驱动的ai

现在的ai好像没有办法判断自己是否逻辑自洽

transformer相当于存储了[问题 + 已经回答的内容] -> [下一个词] 的映射关系. 但是人类拥有自我总结的能力,我们应该会自动对词进行分类,并且构造模型去存储分类之间的映射关系

另外不仅仅是token之间互相有映射关系,句子之间也有,段落之间也有,文章之间也有,这是一个可以不断递归的结构.张量之间的映射关系.

一个向量里每个维度虽然都是用数字表示的,但是这个数字其实只是一个编号,使用来区分同一维度的不同状态,也就是给不同状态一个编码,很多维度的数字是不能比较大小的
一个矩阵有多个列向量组成,我可以给每个列向量一个标量的编码,这样这个矩阵就实现了降维,变成了一个向量.这个方法是可以递归使用的,注意内容相同的列向量编码要相同,编码还是起到区分不同的作用
transformer输入是一个二维张量(矩阵)我总觉得不好,应该输入一个一维的张量(向量),然后当我们需要对向量中某个维度升维的时候,我再针对性升维,需要降维的时候我再降维.

字与字是同层的,词与词是同层的,句子与句子是同层的...不同层之间也有关系,关系还有类型的区别

一个概念的向量维度的本质是: 这个概念和某个概念的一种关系

需要有一个升维模型和降维模型,专门处理把标量向量化,和把向量标量化.这只是升降一个维度的情况,也可以变化n个维度.
[一个向量降维和升维的例子.excalidraw](一个向量降维和升维的例子.excalidraw.md)
升维的目的通常是为了计算两个向量的相关度,但是未必是唯一寻找相关度的方案,相关度这个词总觉得不太准确,因为有些相关是没有程度的概念的,相关态可能准确一点.

大脑很多时候在做这四件事: 
- 选取部分维度  
- 升维
- 降维
- 寻找两个概念的关系

由于transformer的解码器输入的参数是矩阵,而不是向量.所以训练完成后,可以认为神经网络学会了比词语低一维度的概念的映射关系.但是这种映射关系存在多余的维度
例子:
```
- 在样本 A 中：三个点 i,j,k 的值是 (3.5, 7.2, –1)
    
- 在样本 B 中：三个点 i,j,k 的值也是 (3.5, 7.2, –1)
    
- 在样本 C 中：三个点 i,j,k 的值也一样
```
我需要他找出来(3.5, 7.2, –1).这样的话就能找出来主谓宾这种语法的规律.
需要一种能力: 在多个张量中找出里面的共同点. 比如: 若干个坐标上的值总是一样的.
这种能力只是一种发现规律的方式.

即使你发现了这些同步的坐标,你还需要把他们翻译成自然语言.但是每个向量的坐标是似乎是没有意义的,但是其实不是没有意义,而是坐标系需要变化一下,需要把坐标旋转,剪切,放缩,平移.也就是:
```
矩阵*向量+平移向量
```
这样应该可以把某些坐标轴转化成已知概念,如果不在已知概念中,有时还需要为此创造新概念.

这是一种发现规律的方法,但是不一定可靠.还有第二种方法: 就是推理+验证
我们的脑子是可以在一定程度上判断两句话是否矛盾的,比如: 他既是A又不是A.这句话我们在理解的时候会觉得不舒服.不知道这是不是天生的.

理解的本质可能就是创建合适的映射机,保存与一个概念有关的映射关系.